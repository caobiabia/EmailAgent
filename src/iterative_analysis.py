import os
import asyncio
import json
import re
from urllib.parse import urlparse
from crawl4ai import AsyncWebCrawler
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from dotenv import load_dotenv
from logger import logger

# --- 1. åŠ è½½ç¯å¢ƒå˜é‡å’Œåˆå§‹åŒ–LLM ---
load_dotenv()
ali_api_key = os.getenv("DASHSCOPE_API_KEY")

if not ali_api_key:
    logger.error("é”™è¯¯ï¼šè¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®äº† DASHSCOPE_API_KEY")
    exit()

llm = ChatOpenAI(
    model="qwen-turbo",
    temperature=0.2,
    api_key=ali_api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# --- 2. å®šä¹‰LLMåˆ†æç”¨çš„Prompt ---

# å”¯ä¸€ Prompt: è´Ÿè´£æ‰€æœ‰é˜¶æ®µçš„åˆ†æå’Œå†³ç­–
iterative_analysis_prompt_template = """
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¸‚åœºåˆ†æå¸ˆå’Œæ™ºèƒ½ç½‘é¡µçˆ¬è™«ã€‚æˆ‘ä¸ºä½ æä¾›äº†å…¬å¸ç½‘ç«™å·²çˆ¬å–çš„å†…å®¹ã€‚
        
        ä½ çš„ä»»åŠ¡æ˜¯ï¼š
        1.  åˆ†ææ‰€æä¾›çš„å†…å®¹ï¼Œå¯¹å…¬å¸çš„æ ¸å¿ƒä¸šåŠ¡ã€äº§å“ã€æœåŠ¡å’Œä»·å€¼ä¸»å¼ å½¢æˆå…¨é¢çš„ç†è§£ã€‚
        2.  åŸºäºåˆ†æï¼Œåˆ¤æ–­ä¿¡æ¯æ˜¯å¦è¶³ä»¥ç¼–å†™ä¸€å°é«˜åº¦ä¸ªæ€§åŒ–çš„å¼€å‘ä¿¡ï¼ˆcold emailï¼‰ã€‚
        3.  å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œè¯·å»ºè®®ä¸‹ä¸€ä¸ªæœ€ç›¸å…³çš„URLå­è·¯å¾„ã€‚é€‰æ‹©æœ€æœ‰å¯èƒ½æä¾›è¯¦ç»†äº§å“/æœåŠ¡ä¿¡æ¯çš„è·¯å¾„ï¼ˆä¾‹å¦‚ï¼Œ'products', 'solutions', 'services'ï¼‰ã€‚
        4.  å¦‚æœä¿¡æ¯å·²è¶³å¤Ÿï¼Œè¯·å›ç­” "DONE"ï¼Œå¹¶æä¾›ä¸€ä¸ªæœ€ç»ˆè¯¦ç»†çš„æ€»ç»“ã€‚
        
        è¯·ä»¥ç»“æ„åŒ–çš„ JSON æ ¼å¼è¾“å‡ºä½ çš„å“åº”ã€‚
        
        "ç»§ç»­çˆ¬å–"çš„ JSON è¾“å‡ºç¤ºä¾‹:
        {{
            "status": "CONTINUE",
            "summary_so_far": "è¯¥å…¬å¸ä¼¼ä¹æ˜¯ä¸€å®¶ B2B è½¯ä»¶ä¾›åº”å•†ï¼Œä½†å…·ä½“äº§å“ç»†èŠ‚å°šä¸æ¸…æ¥šã€‚ä¸»é¡µæåˆ°äº† 'AI èµ‹èƒ½çš„è§£å†³æ–¹æ¡ˆ'ã€‚",
            "next_url_path": "/solutions"
        }}
        
        "å®Œæˆçˆ¬å–"çš„ JSON è¾“å‡ºç¤ºä¾‹:
        {{
            "status": "DONE",
            "final_analysis": {{
                "company_summary": "å¯¹å…¬å¸ä¸šåŠ¡ã€äº§å“å’ŒæœåŠ¡çš„è¯¦ç»†æ€»ç»“ã€‚",
                "target_market": "è¯†åˆ«å‡ºçš„ç›®æ ‡å®¢æˆ·ç¾¤ä½“ï¼ˆä¾‹å¦‚ï¼š'é›¶å”®ä¸šçš„ä¸­å°å‹ä¼ä¸š'ï¼‰ã€‚",
                "potential_pain_points": ["åˆ—å‡ºå…¬å¸äº§å“/æœåŠ¡ä¸ºå…¶å®¢æˆ·è§£å†³çš„é—®é¢˜ã€‚", "ä¾‹å¦‚ï¼š'ä½æ•ˆçš„åº“å­˜ç®¡ç†'"]
            }}
        }}
        
        ---
        å·²çˆ¬å–çš„å†…å®¹:
        {crawled_content}
        """

analysis_prompt = PromptTemplate(
    template=iterative_analysis_prompt_template,
    input_variables=["crawled_content"]
)
analysis_chain = LLMChain(llm=llm, prompt=analysis_prompt)


def extract_json_from_response(response_text: str):
    """
    å°è¯•ä»å¯èƒ½åŒ…å«é¢å¤–æ–‡æœ¬çš„å­—ç¬¦ä¸²ä¸­æå– JSON å¯¹è±¡ã€‚
    """
    match = re.search(r'\{.*}', response_text, re.DOTALL)
    if match:
        json_string = match.group(0)
        try:
            return json.loads(json_string)
        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æå¤±è´¥: {e}")
            return None
    logger.warning("åœ¨LLMå“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONå¯¹è±¡ã€‚")
    return None


async def ai_company_profiler_iterative(company_url: str, max_crawls=5):
    """
    ä½¿ç”¨å¤šè½® AI-driven çˆ¬å–å’Œåˆ†æï¼Œç›´åˆ°LLMè®¤ä¸ºä¿¡æ¯å·²è¶³å¤Ÿã€‚
    """
    logger.info(f"===== æ­£åœ¨ä¸ºå…¬å¸ {company_url} è¿›è¡ŒAIèƒŒè°ƒ (å¤šè½®æ¨¡å¼)... =====")
    url_path = urlparse(company_url).path
    if '.' in os.path.basename(url_path):
        base_path = os.path.dirname(url_path)
    else:
        base_path = url_path
    if not base_path.endswith('/'):
        base_path += '/'

    parsed_url = urlparse(company_url)
    base_url_for_session = f"{parsed_url.scheme}://{parsed_url.netloc}{base_path}"
    logger.info(f"  -> ä¼šè¯åŸºç¡€URLå·²è®¾å®šä¸º: {base_url_for_session}")

    async with AsyncWebCrawler() as crawler:
        full_content = ""
        current_url = company_url
        crawls_count = 0
        visited_paths = {company_url.strip('/')}

        while crawls_count < max_crawls:
            crawls_count += 1
            logger.info(f"  -> ç¬¬ {crawls_count} æ¬¡å°è¯•: çˆ¬å– {current_url}")

            try:
                crawl_result = await crawler.arun(url=current_url)
                if crawl_result and crawl_result.markdown:
                    full_content += f"\n\n--- Content from {current_url} ---\n\n{crawl_result.markdown}"
                else:
                    logger.warning(f"  -> çˆ¬å– {current_url} æœªè¿”å›æœ‰æ•ˆå†…å®¹ã€‚")

            except Exception as e:
                logger.error(f"  -> é”™è¯¯: çˆ¬å– {current_url} å¤±è´¥: {e}")
                break

            try:
                logger.info("  -> æ­£åœ¨è¿›è¡ŒAIåˆ†æå¹¶å†³ç­–ä¸‹ä¸€æ­¥...")
                analysis_response = await analysis_chain.ainvoke({"crawled_content": full_content})
                analysis_data = extract_json_from_response(analysis_response['text'])

                if analysis_data is None:
                    logger.error("  -> é”™è¯¯: LLMè¿”å›äº†éJSONæ ¼å¼å“åº”ï¼Œæ— æ³•è§£æã€‚ç»ˆæ­¢å¾ªç¯ã€‚")
                    break

                status = analysis_data.get("status")
                logger.info(f"  -> AI Agent çŠ¶æ€: {status}")
                logger.info(f"  -> AI Agent å½“å‰æ€»ç»“: {analysis_data.get('summary_so_far', 'N/A')}")

                if status == "DONE":
                    logger.success("  -> AI Agentè®¤ä¸ºä¿¡æ¯å·²è¶³å¤Ÿï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚")
                    return analysis_data.get("final_analysis")

                elif status == "CONTINUE":
                    next_path = analysis_data.get("next_url_path")
                    if next_path:
                        next_url = base_url_for_session + next_path.lstrip('/')
                        current_url = next_url
                        visited_paths.add(current_url.strip('/'))
                        logger.info(f"  -> AI Agentå†³å®šç»§ç»­çˆ¬å–ï¼Œä¸‹ä¸€ä¸ªç›®æ ‡æ˜¯: {current_url}")
                    else:
                        logger.warning("  -> AI Agentå†³å®šç»§ç»­ï¼Œä½†æ²¡æœ‰æä¾›ä¸‹ä¸€ä¸ªè·¯å¾„ï¼Œç»ˆæ­¢å¾ªç¯ã€‚")
                        break
                else:
                    logger.error(f"  -> AIè¿”å›äº†æœªçŸ¥çš„çŠ¶æ€: {status}ï¼Œç»ˆæ­¢å¾ªç¯ã€‚")
                    break

            except Exception as e:
                logger.error(f"  -> é”™è¯¯: åˆ†ææˆ–å†³ç­–å¤±è´¥: {e}")
                break

        logger.warning("  -> å¾ªç¯ç»“æŸï¼Œä½¿ç”¨ç°æœ‰å†…å®¹è¿›è¡Œæœ€ç»ˆåˆ†æã€‚")
        final_analysis_prompt_template = """
        ä½ æ˜¯ä¸€ä½é«˜çº§å¸‚åœºåˆ†æå¸ˆã€‚åŸºäºä»å…¬å¸ç½‘ç«™ä¸Šçˆ¬å–çš„æ‰€æœ‰å†…å®¹ï¼Œä½ çš„ä»»åŠ¡æ˜¯æä¾›ä¸€ä»½æœ€ç»ˆçš„ã€å…¨é¢çš„åˆ†ææŠ¥å‘Šã€‚
        ä½ çš„ç›®æ ‡æ˜¯æ”¶é›†è¶³å¤Ÿçš„æƒ…æŠ¥ï¼Œç”¨äºæ’°å†™ä¸€å°æœ‰é’ˆå¯¹æ€§çš„é”€å”®é‚®ä»¶ã€‚

        è¯·æ‰§è¡Œä»¥ä¸‹åˆ†æå¹¶ä»¥ JSON æ ¼å¼ä½¿ç”¨ä¸­æ–‡è¾“å‡ºä½ çš„æŠ¥å‘Šï¼š
        1.  **å…¬å¸æ€»ç»“ (company_summary)**: ç®€æ´åœ°æ€»ç»“å…¬å¸åšä»€ä¹ˆï¼Œå…¶æ ¸å¿ƒäº§å“/æœåŠ¡ï¼Œä»¥åŠä¸»è¦ä»·å€¼ä¸»å¼ ã€‚
        2.  **æ ¸å¿ƒä¸šåŠ¡/äº§å“ (core_products_services)**: è¯¦ç»†æè¿°å…¬å¸çš„ä¸»è¦ä¸šåŠ¡çº¿æˆ–æ ¸å¿ƒäº§å“/æœåŠ¡ã€‚
        3.  **ç›®æ ‡å¸‚åœº (target_market)**: æè¿°å…¬å¸çš„ç†æƒ³å®¢æˆ·ç”»åƒæˆ–ç›®æ ‡è¡Œä¸šã€‚
        4.  **æ½œåœ¨ç—›ç‚¹ (potential_pain_points)**: åŸºäºå…¶äº§å“/æœåŠ¡ï¼Œåˆ—å‡ºå…¶å®¢æˆ·å¯èƒ½é¢ä¸´çš„ã€è€Œå…¶äº§å“/æœåŠ¡æ—¨åœ¨è§£å†³çš„æ½œåœ¨é—®é¢˜æˆ–æŒ‘æˆ˜ã€‚
        5.  **æ½œåœ¨åˆä½œç‚¹ (potential_collaboration_points)**: åŸºäºå…¬å¸çš„ä¸šåŠ¡å’Œä½ çš„æ´å¯Ÿï¼Œæå‡ºå‡ ä¸ªå¯èƒ½çš„åˆä½œæ–¹å‘æˆ–åˆ‡å…¥ç‚¹ï¼Œç”¨äºåœ¨å¼€å‘ä¿¡ä¸­æåŠã€‚

        å·²çˆ¬å–çš„å†…å®¹:
        {all_content}

        è¯·ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼æä¾›ä½ çš„ä¸­æ–‡è¾“å‡ºï¼š
        {{
            "company_summary": "...",
            "core_products_services": "...",
            "target_market": "...",
            "potential_pain_points": ["...", "..."],
            "potential_collaboration_points": ["...", "..."]
        }}
        """
        final_analysis_prompt = PromptTemplate(
            template=final_analysis_prompt_template,
            input_variables=["all_content"]
        )
        final_analysis_chain = LLMChain(llm=llm, prompt=final_analysis_prompt)

        try:
            final_response = await final_analysis_chain.ainvoke({"all_content": full_content})
            final_analysis_data = extract_json_from_response(final_response['text'])
            return final_analysis_data or {"error": "æœ€ç»ˆåˆ†æå¤±è´¥ï¼Œæ— æ³•è§£æã€‚"}
        except Exception as e:
            logger.error(f"  -> æœ€ç»ˆåˆ†æç¯èŠ‚å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": f"æœ€ç»ˆåˆ†ææ—¶å‘ç”Ÿå¼‚å¸¸: {e}"}


def pretty_print_analysis(analysis_data: dict):
    """
    æ ¼å¼åŒ–å¹¶æ‰“å°æœ€ç»ˆçš„å…¬å¸åˆ†ææŠ¥å‘Šã€‚
    """
    if not analysis_data or "error" in analysis_data:
        logger.error(f"æœªèƒ½ç”Ÿæˆåˆ†ææŠ¥å‘Š: {analysis_data.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return

    print("\n\n" + "=" * 25 + " AI å…¬å¸èƒŒè°ƒæŠ¥å‘Š " + "=" * 25)

    print("\n## ğŸ¢ å…¬å¸æ€»ç»“")
    print(analysis_data.get("company_summary", "æ— "))

    print("\n## ğŸ’¡ æ ¸å¿ƒä¸šåŠ¡/äº§å“")
    print(analysis_data.get("core_products_services", "æ— "))

    print("\n## ğŸ¯ ç›®æ ‡å¸‚åœº")
    print(analysis_data.get("target_market", "æ— "))

    print("\n## âš¡ï¸ æ½œåœ¨ç—›ç‚¹")
    pain_points = analysis_data.get("potential_pain_points", [])
    if pain_points:
        for point in pain_points:
            print(f"  - {point}")
    else:
        print("æ— ")

    print("\n## ğŸ¤ æ½œåœ¨åˆä½œç‚¹")
    collaboration_points = analysis_data.get("potential_collaboration_points", [])
    if collaboration_points:
        for point in collaboration_points:
            print(f"  - {point}")
    else:
        print("æ— ")

    print("\n" + "=" * 65 + "\n")


# --- 4. ä¸»ç¨‹åºå…¥å£ ---
async def run_profiler(url: str):
    """
    æ‰§è¡Œåˆ†æå™¨å¹¶æ‰“å°ç»“æœã€‚
    """
    analysis_result = await ai_company_profiler_iterative(url, max_crawls=3)
    if analysis_result:
        pretty_print_analysis(analysis_result)
    else:
        logger.error("åˆ†æè¿‡ç¨‹æœªè¿”å›ä»»ä½•ç»“æœã€‚")


if __name__ == "__main__":
    # æƒ³è¦åˆ†æçš„å…¬å¸ç½‘å€
    TARGET_URL = "https://www.aceler.com.cn/"

    # è¿è¡Œå¼‚æ­¥ä¸»ç¨‹åº
    asyncio.run(run_profiler(TARGET_URL))